{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¹ Fabric Retail Data Cleansing and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¥ Load Bronze (raw) JSON
",
    "df_raw = spark.read.option("multiline", "true").json("Files/raw/retail/retail_raw_data_560.json")
",
    "df_raw.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§¼ Clean and Normalize Data (Silver Layer)
",
    "from pyspark.sql.functions import *
",
    "
",
    "df_clean = (
",
    "    df_raw
",
    "    .withColumnRenamed("Order iD", "OrderID")
",
    "    .withColumnRenamed("order_date", "OrderDate")
",
    "    .withColumnRenamed("cust_ID", "CustomerID")
",
    "    .withColumnRenamed("Cust Segment", "CustomerSegment")
",
    "    .withColumnRenamed("region", "Region")
",
    "    .withColumnRenamed("prodct_ID", "ProductID")
",
    "    .withColumnRenamed("Prodct Category", "ProductCategory")
",
    "    .withColumnRenamed("prod_Name", "ProductName")
",
    "    .withColumnRenamed("QTY", "Quantity")
",
    "    .withColumnRenamed("price_unit", "UnitPrice")
",
    "    .withColumnRenamed("cost_unit", "CostPrice")
",
    "    .withColumnRenamed("channel", "Channel")
",
    "    .withColumnRenamed("new customer", "IsNewCustomer")
",
    "    .withColumnRenamed("store type", "StoreType")
",
    "    .withColumn("OrderDate", to_date("OrderDate", "dd-MM-yyyy"))
",
    "    .withColumn("Quantity", col("Quantity").cast("int"))
",
    "    .withColumn("UnitPrice", col("UnitPrice").cast("double"))
",
    "    .withColumn("CostPrice", col("CostPrice").cast("double"))
",
    "    .dropna(subset=["OrderID", "OrderDate", "CustomerID", "ProductID"])
",
    ")
",
    "
",
    "# Save to Silver
",
    "df_clean.write.mode("overwrite").saveAsTable("silver_retail_cleaned")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Aggregate for Gold Table (segmentation based)
",
    "df_gold = (
",
    "    df_clean
",
    "    .withColumn("Year", year("OrderDate"))
",
    "    .withColumn("Quarter", quarter("OrderDate"))
",
    "    .withColumn("Month", month("OrderDate"))
",
    "    .withColumn("SalesAmount", col("Quantity") * col("UnitPrice"))
",
    "    .withColumn("CostAmount", col("Quantity") * col("CostPrice"))
",
    "    .withColumn("Profit", col("SalesAmount") - col("CostAmount"))
",
    "    .groupBy("CustomerSegment", "ProductCategory", "Region", "Channel", "StoreType", "Year", "Quarter", "Month")
",
    "    .agg(
",
    "        sum("SalesAmount").alias("TotalSales"),
",
    "        sum("CostAmount").alias("TotalCost"),
",
    "        sum("Profit").alias("GrossProfit"),
",
    "        countDistinct("OrderID").alias("OrderCount"),
",
    "        countDistinct("CustomerID").alias("CustomerCount")
",
    "    )
",
    "    .withColumn("AvgOrderValue", col("TotalSales") / col("OrderCount"))
",
    "    .withColumn("ProfitMargin", (col("GrossProfit") / col("TotalSales")) * 100)
",
    ")
",
    "
",
    "# Save to Gold
",
    "df_gold.write.mode("overwrite").saveAsTable("gold_retail_segmented_metrics")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Spark 3.4.1)", 
   "language": "python",
   "name": "pyspark3.4"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
